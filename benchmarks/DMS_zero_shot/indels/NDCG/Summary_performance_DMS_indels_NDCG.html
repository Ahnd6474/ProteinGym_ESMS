<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model_name</th>
      <th>Model type</th>
      <th>Average_NDCG</th>
      <th>Bootstrap_standard_error_NDCG</th>
      <th>Function_Activity</th>
      <th>Function_Binding</th>
      <th>Function_Expression</th>
      <th>Function_OrganismalFitness</th>
      <th>Function_Stability</th>
      <th>Low_MSA_depth</th>
      <th>Medium_MSA_depth</th>
      <th>High_MSA_depth</th>
      <th>Taxa_Human</th>
      <th>Taxa_Other_Eukaryote</th>
      <th>Taxa_Prokaryote</th>
      <th>Taxa_Virus</th>
      <th>Model details</th>
      <th>References</th>
    </tr>
    <tr>
      <th>Model_rank</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>PoET (200M)</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.767</td>
      <td>0.000</td>
      <td>0.808</td>
      <td>NaN</td>
      <td>0.801</td>
      <td>0.650</td>
      <td>0.807</td>
      <td>0.629</td>
      <td>0.816</td>
      <td>0.790</td>
      <td>0.804</td>
      <td>0.791</td>
      <td>0.767</td>
      <td>0.831</td>
      <td>PoET (200M)</td>
      <td>&lt;a href='https://papers.nips.cc/paper_files/paper/2023/hash/f4366126eba252699b280e8f93c0ab2f-Abstract-Conference.html'&gt;Truong, Timothy F. and Tristan Bepler. PoET: A generative model of protein families as sequences-of-sequences. NeurIPS.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Progen2 M</td>
      <td>Protein language model</td>
      <td>0.763</td>
      <td>0.016</td>
      <td>0.796</td>
      <td>NaN</td>
      <td>0.820</td>
      <td>0.595</td>
      <td>0.840</td>
      <td>0.642</td>
      <td>0.789</td>
      <td>0.861</td>
      <td>0.878</td>
      <td>0.815</td>
      <td>0.775</td>
      <td>0.694</td>
      <td>Progen2 medium model (760M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Progen2 Base</td>
      <td>Protein language model</td>
      <td>0.751</td>
      <td>0.017</td>
      <td>0.812</td>
      <td>NaN</td>
      <td>0.814</td>
      <td>0.568</td>
      <td>0.811</td>
      <td>0.623</td>
      <td>0.751</td>
      <td>0.844</td>
      <td>0.880</td>
      <td>0.795</td>
      <td>0.674</td>
      <td>0.701</td>
      <td>Progen2 base model (760M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Progen2 XL</td>
      <td>Protein language model</td>
      <td>0.751</td>
      <td>0.013</td>
      <td>0.765</td>
      <td>NaN</td>
      <td>0.780</td>
      <td>0.580</td>
      <td>0.878</td>
      <td>0.718</td>
      <td>0.834</td>
      <td>0.872</td>
      <td>0.882</td>
      <td>0.864</td>
      <td>0.784</td>
      <td>0.808</td>
      <td>Progen2 xlarge model (6.4B params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Progen2 L</td>
      <td>Protein language model</td>
      <td>0.748</td>
      <td>0.016</td>
      <td>0.781</td>
      <td>NaN</td>
      <td>0.802</td>
      <td>0.571</td>
      <td>0.839</td>
      <td>0.637</td>
      <td>0.791</td>
      <td>0.854</td>
      <td>0.870</td>
      <td>0.802</td>
      <td>0.793</td>
      <td>0.693</td>
      <td>Progen2 large model (2.7B params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Tranception L no retrieval</td>
      <td>Protein language model</td>
      <td>0.744</td>
      <td>0.008</td>
      <td>0.790</td>
      <td>NaN</td>
      <td>0.786</td>
      <td>0.643</td>
      <td>0.755</td>
      <td>0.724</td>
      <td>0.730</td>
      <td>0.767</td>
      <td>0.790</td>
      <td>0.686</td>
      <td>0.739</td>
      <td>0.811</td>
      <td>Tranception Large model (700M params) without retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Tranception M no retrieval</td>
      <td>Protein language model</td>
      <td>0.741</td>
      <td>0.012</td>
      <td>0.798</td>
      <td>NaN</td>
      <td>0.803</td>
      <td>0.645</td>
      <td>0.719</td>
      <td>0.640</td>
      <td>0.695</td>
      <td>0.747</td>
      <td>0.744</td>
      <td>0.698</td>
      <td>0.704</td>
      <td>0.715</td>
      <td>Tranception Medium model (300M params) without retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Provean</td>
      <td>Alignment-based model</td>
      <td>0.740</td>
      <td>0.015</td>
      <td>0.759</td>
      <td>NaN</td>
      <td>0.770</td>
      <td>0.580</td>
      <td>0.851</td>
      <td>0.706</td>
      <td>0.822</td>
      <td>0.838</td>
      <td>0.836</td>
      <td>0.848</td>
      <td>0.771</td>
      <td>0.829</td>
      <td>Provean model</td>
      <td>&lt;a href='https://www.jcvi.org/publications/predicting-functional-effect-amino-acid-substitutions-and-indels'&gt;Choi Y, Sims GE, Murphy S, Miller JR, Chan AP (2012). Predicting the functional effect of amino acid substitutions and indels. PloS one.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>9</th>
      <td>RITA XL</td>
      <td>Protein language model</td>
      <td>0.740</td>
      <td>0.009</td>
      <td>0.746</td>
      <td>NaN</td>
      <td>0.782</td>
      <td>0.612</td>
      <td>0.821</td>
      <td>0.718</td>
      <td>0.769</td>
      <td>0.838</td>
      <td>0.869</td>
      <td>0.769</td>
      <td>0.735</td>
      <td>0.788</td>
      <td>RITA xlarge model (1.2B params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2205.05789'&gt;Hesslow, D., Zanichelli, N., Notin, P., Poli, I., &amp; Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>10</th>
      <td>RITA L</td>
      <td>Protein language model</td>
      <td>0.736</td>
      <td>0.013</td>
      <td>0.780</td>
      <td>NaN</td>
      <td>0.796</td>
      <td>0.564</td>
      <td>0.805</td>
      <td>0.626</td>
      <td>0.745</td>
      <td>0.835</td>
      <td>0.856</td>
      <td>0.770</td>
      <td>0.689</td>
      <td>0.773</td>
      <td>RITA large model (680M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2205.05789'&gt;Hesslow, D., Zanichelli, N., Notin, P., Poli, I., &amp; Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>11</th>
      <td>TranceptEVE L</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.727</td>
      <td>0.011</td>
      <td>0.775</td>
      <td>NaN</td>
      <td>0.770</td>
      <td>0.641</td>
      <td>0.722</td>
      <td>0.731</td>
      <td>0.685</td>
      <td>0.749</td>
      <td>0.742</td>
      <td>0.703</td>
      <td>0.679</td>
      <td>0.774</td>
      <td>TranceptEVE Large model (Tranception Large &amp; retrieved EVE model)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'&gt;Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. &amp; Marks, D.S. &amp;  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>12</th>
      <td>TranceptEVE M</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.726</td>
      <td>0.014</td>
      <td>0.788</td>
      <td>NaN</td>
      <td>0.786</td>
      <td>0.652</td>
      <td>0.679</td>
      <td>0.669</td>
      <td>0.640</td>
      <td>0.725</td>
      <td>0.707</td>
      <td>0.686</td>
      <td>0.649</td>
      <td>0.667</td>
      <td>TranceptEVE Medium model (Tranception Medium &amp; retrieved EVE model)</td>
      <td>&lt;a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'&gt;Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. &amp; Marks, D.S. &amp;  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>13</th>
      <td>RITA M</td>
      <td>Protein language model</td>
      <td>0.726</td>
      <td>0.015</td>
      <td>0.767</td>
      <td>NaN</td>
      <td>0.795</td>
      <td>0.552</td>
      <td>0.788</td>
      <td>0.605</td>
      <td>0.719</td>
      <td>0.831</td>
      <td>0.842</td>
      <td>0.732</td>
      <td>0.698</td>
      <td>0.773</td>
      <td>RITA medium model (300M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2205.05789'&gt;Hesslow, D., Zanichelli, N., Notin, P., Poli, I., &amp; Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Tranception L</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.721</td>
      <td>0.011</td>
      <td>0.772</td>
      <td>NaN</td>
      <td>0.768</td>
      <td>0.642</td>
      <td>0.701</td>
      <td>0.718</td>
      <td>0.671</td>
      <td>0.728</td>
      <td>0.727</td>
      <td>0.684</td>
      <td>0.656</td>
      <td>0.766</td>
      <td>Tranception Large model (700M params) with retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Progen2 S</td>
      <td>Protein language model</td>
      <td>0.718</td>
      <td>0.018</td>
      <td>0.766</td>
      <td>NaN</td>
      <td>0.784</td>
      <td>0.549</td>
      <td>0.773</td>
      <td>0.594</td>
      <td>0.702</td>
      <td>0.821</td>
      <td>0.823</td>
      <td>0.701</td>
      <td>0.724</td>
      <td>0.745</td>
      <td>Progen2 small model (150M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2206.13517'&gt; Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., &amp; Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. &lt;/a&gt;</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Tranception S no retrieval</td>
      <td>Protein language model</td>
      <td>0.706</td>
      <td>0.011</td>
      <td>0.763</td>
      <td>NaN</td>
      <td>0.788</td>
      <td>0.625</td>
      <td>0.649</td>
      <td>0.710</td>
      <td>0.623</td>
      <td>0.679</td>
      <td>0.693</td>
      <td>0.597</td>
      <td>0.665</td>
      <td>0.675</td>
      <td>Tranception Small model (85M params) without retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Wavenet</td>
      <td>Alignment-based model</td>
      <td>0.693</td>
      <td>0.031</td>
      <td>0.703</td>
      <td>NaN</td>
      <td>0.709</td>
      <td>0.539</td>
      <td>0.823</td>
      <td>0.627</td>
      <td>0.818</td>
      <td>0.791</td>
      <td>0.802</td>
      <td>0.811</td>
      <td>0.750</td>
      <td>0.828</td>
      <td>Wavenet model</td>
      <td>&lt;a href='https://www.nature.com/articles/s41467-021-22732-w'&gt;Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., &amp; Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Tranception M</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.691</td>
      <td>0.019</td>
      <td>0.752</td>
      <td>NaN</td>
      <td>0.728</td>
      <td>0.616</td>
      <td>0.667</td>
      <td>0.684</td>
      <td>0.628</td>
      <td>0.704</td>
      <td>0.695</td>
      <td>0.670</td>
      <td>0.620</td>
      <td>0.674</td>
      <td>Tranception Medium model (300M params) with retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>19</th>
      <td>TranceptEVE S</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.688</td>
      <td>0.012</td>
      <td>0.747</td>
      <td>NaN</td>
      <td>0.766</td>
      <td>0.634</td>
      <td>0.605</td>
      <td>0.738</td>
      <td>0.577</td>
      <td>0.644</td>
      <td>0.646</td>
      <td>0.595</td>
      <td>0.600</td>
      <td>0.624</td>
      <td>TranceptEVE Small model (Tranception Small &amp; retrieved EVE model)</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>20</th>
      <td>RITA S</td>
      <td>Protein language model</td>
      <td>0.687</td>
      <td>0.015</td>
      <td>0.699</td>
      <td>NaN</td>
      <td>0.748</td>
      <td>0.579</td>
      <td>0.721</td>
      <td>0.669</td>
      <td>0.659</td>
      <td>0.761</td>
      <td>0.759</td>
      <td>0.687</td>
      <td>0.666</td>
      <td>0.695</td>
      <td>RITA small model (85M params)</td>
      <td>&lt;a href='https://arxiv.org/abs/2205.05789'&gt;Hesslow, D., Zanichelli, N., Notin, P., Poli, I., &amp; Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>21</th>
      <td>Tranception S</td>
      <td>Hybrid - Alignment &amp; PLM</td>
      <td>0.677</td>
      <td>0.015</td>
      <td>0.741</td>
      <td>NaN</td>
      <td>0.757</td>
      <td>0.617</td>
      <td>0.591</td>
      <td>0.726</td>
      <td>0.564</td>
      <td>0.627</td>
      <td>0.630</td>
      <td>0.581</td>
      <td>0.583</td>
      <td>0.622</td>
      <td>Tranception Small model (85M params) with retrieval</td>
      <td>&lt;a href='https://proceedings.mlr.press/v162/notin22a.html'&gt;Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., &amp; Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>22</th>
      <td>Hidden Markov Model</td>
      <td>Alignment-based model</td>
      <td>0.642</td>
      <td>0.034</td>
      <td>0.672</td>
      <td>NaN</td>
      <td>0.660</td>
      <td>0.529</td>
      <td>0.708</td>
      <td>0.549</td>
      <td>0.740</td>
      <td>0.666</td>
      <td>0.728</td>
      <td>0.660</td>
      <td>0.668</td>
      <td>0.734</td>
      <td>Profile Hidden Markov model</td>
      <td>&lt;a href='http://hmmer.org/'&gt;HMMER: biosequence analysis using profile hidden Markov models&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>23</th>
      <td>ProtGPT2</td>
      <td>Protein language model</td>
      <td>0.537</td>
      <td>0.038</td>
      <td>0.519</td>
      <td>NaN</td>
      <td>0.652</td>
      <td>0.344</td>
      <td>0.634</td>
      <td>0.526</td>
      <td>0.575</td>
      <td>0.645</td>
      <td>0.671</td>
      <td>0.589</td>
      <td>0.525</td>
      <td>0.610</td>
      <td>ProtGPT2 model</td>
      <td>&lt;a href='https://www.nature.com/articles/s41467-022-32007-7'&gt;Ferruz, N., Schmidt, S., &amp; Höcker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.&lt;/a&gt;</td>
    </tr>
    <tr>
      <th>24</th>
      <td>Unirep</td>
      <td>Protein language model</td>
      <td>0.527</td>
      <td>0.045</td>
      <td>0.526</td>
      <td>NaN</td>
      <td>0.656</td>
      <td>0.329</td>
      <td>0.599</td>
      <td>0.504</td>
      <td>0.558</td>
      <td>0.599</td>
      <td>0.571</td>
      <td>0.559</td>
      <td>0.553</td>
      <td>0.707</td>
      <td>Unirep model</td>
      <td>&lt;a href='https://www.nature.com/articles/s41592-019-0598-1'&gt;Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., &amp; Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.&lt;/a&gt;</td>
    </tr>
  </tbody>
</table>